{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805d21c7",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edbd22",
   "metadata": {},
   "source": [
    "Gradient Descent ia a generic optimization algorithm capable of finding optimal solution to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function\n",
    "\n",
    "Gradient Descent measures the local gradient of error function relate to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, we have reached a minimum\n",
    "\n",
    "Concretely, We start by filling $\\theta$ with random values (this is called random initialization). Then we improve it gradually, taking one baby step at a time, each step attempting to descrease the cost function, until the algorithm converges to a minimum\n",
    "\n",
    "An important parameter in GD is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time\n",
    "\n",
    "On the other hand, if the learning rate is too high, it might jump across the valley and end up on the other side. This might make the algorithm diverge, with larger and larger values, failing to find a good solution\n",
    "\n",
    "Finally, not all cost functions look like nice, regular bowls. If the random initialization starts the algorithm on the left, then it will converge to a *local minimum*, which is not as good as the *global minimum*. If it starts on the right, then it will take a very long time to cross the plateau. And if we stop too early, we will never reach the global minimum\n",
    "\n",
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if we pick any two points on the curve, the line joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. Gradient Descent is guaranteed to close the global minimum (if we wait long enough and if the learning rate is not too high)\n",
    "\n",
    "**Note**: When using Gradient Descent, we should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f8961",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814a9de",
   "metadata": {},
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole\n",
    "training set to compute the gradients at every step, which makes it very slow when\n",
    "the training set is large. At the opposite extreme, Stochastic Gradient Descent just\n",
    "picks a random instance in the training set at every step and computes the gradients\n",
    "based only on that single instance. Obviously this makes the algorithm much faster\n",
    "since it has very little data to manipulate at every iteration. It also makes it possible to\n",
    "train on huge training sets, since only one instance needs to be in memory at each\n",
    "iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be237b3d",
   "metadata": {},
   "source": [
    "To perform Linear Regression using SGD with Scikit-Learn, we can use the SGDRegressor class, which defaults to optimizing the squared error cost function. The fol‐\n",
    "lowing code runs 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the default learning schedule, and it does not use any regularization (penalty=None)\n",
    "\n",
    "**Note**: \n",
    "   - We iterate by rounds of m iterations, each round is called an *epoch*\n",
    "   - The function that determines the learning rate at each iteration is called the learning schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75793e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e6d321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.12734872]), array([3.00022504]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
