{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4E6FxXY91eS"
   },
   "source": [
    "If we aggregate the predictions of a group of predictors (such as classifiers or regressors), we will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble, thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithms is called an Ensemble method.\n",
    "\n",
    "For example, we can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, we obtain the predictions of all the individual trees, then predict the class that gets the most votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDO9bj7UO6pm"
   },
   "source": [
    "## Voting Classifier\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "This majority-vote classifier is called a hard voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1164,
     "status": "ok",
     "timestamp": 1697774299048,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "MchNrNyCO5GB"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),('rf', rnd_clf),('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1697774326610,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "x4tJRJK6O8mP"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "moon_dataset = make_moons(n_samples=10000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(moon_dataset[0], moon_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "executionInfo": {
     "elapsed": 2266,
     "status": "ok",
     "timestamp": 1697774339546,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "3ETXdXVfPE6l",
    "outputId": "c79cdb3e-5016-45be-9a46-7f5fb135e238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5773,
     "status": "ok",
     "timestamp": 1697774357008,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "GxJTGkxPPHnW",
    "outputId": "8924b8b2-b89c-46d6-b3c1-accb80a63d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8268\n",
      "RandomForestClassifier 0.8336\n",
      "SVC 0.8496\n",
      "VotingClassifier 0.8472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "  clf.fit(X_train,y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEufllYRQmbt"
   },
   "source": [
    "## Soft Voting\n",
    "\n",
    "If all classifiers are able to estimate class probabilities (they all have predict_proba() method), then we can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is call soft voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq4Hplx0PW0D"
   },
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms. Another approach is to use the same training algorithm for every predictor and train them on different random subsets of training set. When sampling is performed with replacement, this method called \"bagging\" (short for bootstrap aggregating). When sampling is performed without replacement, it is called \"pasting\"\n",
    "\n",
    "Both bagging and pasting also allow training instances to be sampled several time accross multiple predictors. But only bagging allow training instances to be sampled several time for same predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IoH1KJjTrlX"
   },
   "source": [
    "Once all predictors are trained, the ensemble can make predictions for a new instance by aggregating all predictions of predictors. The aggregation function is statistical mode for classification or average for regression. Each individual predictor has a higher bias than if it were trained on the original training set. Aggregation has a similar bias but lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX380RJJTvA_"
   },
   "source": [
    "## Bagging and Pasting in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a simple API for both bagging and pasting with the  BaggingClassifier class (or BaggingRegressor for regression).\n",
    "\n",
    "This is an example of bagging, for pasting, set bootstrap=False\n",
    "\n",
    "The following code trains an ensemble of 500 Decision Tree classifiers: each is trained on 100 training instances randomly sampled from the training set with replacement, n_jobs=-1 to use all available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1697776252200,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "rzg926cbT01-",
    "outputId": "f6ecc9bc-67b8-49df-a556-2093290a8267"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bspu6XAEhpS"
   },
   "source": [
    "BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (e.g, if it has predict_proba() method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQe7F9uqFEVT"
   },
   "source": [
    "## Out of Bag Evaluation\n",
    "\n",
    "With Bagging, some instances may be sampled severals times for any even predictor, while others may not be sampled at all. The remaining of the training instances that are not sampled are called out-of-bag (oob) instances.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a seperate validation set. We can evaluate the ensemble itself by averaging out the oob evaluations of each predictor\n",
    "\n",
    "In Scikit-Learn, we can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training. The resulting evaluation score is available through the oob_score_ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1697776256012,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "cLt4Y5l0VYk_",
    "outputId": "f08117cf-8d80-46da-c869-6cf5c3719140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8589333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcznmjqwJZJQ"
   },
   "source": [
    "Let's verify this by using accuracy_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1697776263968,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "FYBeAkQDVwr3",
    "outputId": "a397a43f-68bb-44e3-943e-386b1750961d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KUTm0BMJ3lb"
   },
   "source": [
    "We also have the oob decision function for each training instance that available through the oob_decision_function_ variable. In this case, it returns class probabilities for each training instances.\n",
    "\n",
    "For example, it estimates that the first training instance has a 94.3% belong to the positive class and 5.6% belong to the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1697776299012,
     "user": {
      "displayName": "điệp phạm",
      "userId": "17907400285807461038"
     },
     "user_tz": -420
    },
    "id": "aj-UB8KiWGHO",
    "outputId": "fde64119-b795-40dd-d252-9318fc6065e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05645161, 0.94354839],\n",
       "       [0.01417004, 0.98582996],\n",
       "       [0.91549296, 0.08450704],\n",
       "       ...,\n",
       "       [0.01622718, 0.98377282],\n",
       "       [0.38742394, 0.61257606],\n",
       "       [0.17611336, 0.82388664]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "724_5fUn9z1E"
   },
   "source": [
    "## Random Patches method: \n",
    "\n",
    "Sampling both training instances and feature\n",
    "\n",
    "The BaggingClassifier class supports sampling features as well. Sampling is controlled by 2 hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This technique is useful for high-dimensional inputs (such as image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St872ZQVAZ7Z"
   },
   "source": [
    "## Random Subspaces method:\n",
    "\n",
    "Keeping all training instance (by setting bootstrap=False and max_samples=1.0) but sampling features (by setting bootstrap_features=True and/or max_features <1.0)\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6jJTtupCOu5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCP42rGrhJUqNtLH9TMqZb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
